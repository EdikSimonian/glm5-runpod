# glm5-runpod
One-script deploy of GLM-5 (Q4_K_M) on RunPod with llama.cpp + CUDA
